To achieve the desired batch processing while keeping the original implementation mostly intact, you can introduce a method that handles the batching logic. This method will split the list of filters into sublists, process each batch with the original `storeDataInTXT` method, and concatenate the results.

Here's how you can modify the existing code to support batch processing:

### Step 1: Create a New Method for Batch Processing
Introduce a new method called `processInBatches` in the `PandaAccountService` class to handle the batching logic.

```java
private String processInBatches(String graphQLQuery, Map<String, Object> filters, Map<String, String> eventTypeMap,
                                LoadDto loadDto, String queryName, String currentFilter, List<String> keys) throws BusinessException {
    // Convert the filters to a list of IDs (assuming filters contain IDs)
    List<String> allIds = (List<String>) filters.get(currentFilter); 
    List<String> subList;
    int batchSize = 100;
    StringBuilder result = new StringBuilder();
    boolean isFirstBatch = true;

    for (int i = 0; i < allIds.size(); i += batchSize) {
        // Create a sublist of IDs for the current batch
        subList = allIds.subList(i, Math.min(i + batchSize, allIds.size()));
        
        // Prepare the filters for the current batch
        Map<String, Object> subListFilters = new HashMap<>(filters);
        subListFilters.put(currentFilter, subList);

        // Fire the query for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(graphQLQuery, subListFilters);

        // Convert response to JSON string
        ObjectMapper objectMapper = new ObjectMapper();
        String graphQLResponse;
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // Pass the response to storeDataInTXT for processing
        String batchResult = storeDataInTXT(graphQLResponse, keys, eventTypeMap, loadDto, queryName, currentFilter, isFirstBatch);

        // Append the batch result to the overall result
        result.append(batchResult);
        
        // After the first batch, disable header writing
        isFirstBatch = false;
    }

    return result.toString();
}
```

### Step 2: Modify `storeDataInTXT` to Support Batch Processing
Update the `storeDataInTXT` method to include a flag that controls whether headers are written or not.

```java
public String storeDataInTXT(String jsonResponse, List<String> keys, Map<String, String> eventTypeMap,
    LoadDto loadDto, String queryName, final String currentFilter, boolean writeHeaders) {

    try (StringWriter stringWriter = new StringWriter()) {
        // Parse JSON data
        JSONObject jsonObject = new JSONObject(jsonResponse);
        JSONArray jsonArray;
        if (containEdges) {
            jsonArray = jsonObject.getJSONObject("data").getJSONObject(queryName).getJSONArray("edges");
        } else {
            jsonArray = jsonObject.getJSONObject("data").getJSONArray(queryName);
        }
        Map<String, Boolean> accountsFound = new HashMap<>();

        // Write headers only if writeHeaders is true
        if (writeHeaders) {
            List<String> headers = new ArrayList<>(keys);
            headers.add("eventType");

            stringWriter.write(String.join("\t", headers));
            stringWriter.write("\n");

            BufferedWriter writer = new BufferedWriter(new FileWriter(queryName + ".csv"));
            writer.write(String.join(",", keys));
            writer.newLine();
            writer.close();
        }

        BufferedWriter writer = new BufferedWriter(new FileWriter(queryName + ".csv", true)); // Append mode
        // Iterate over the accounts in the JSON response
        for (int i = 0; i < jsonArray.length(); i++) {
            JSONObject account = jsonArray.getJSONObject(i);
            String accountId = null;
            List<Integer> nestedArrSizes = new ArrayList<>();

            // Get the size of all the nested JSONArray present inside this account
            for (String key : keys) {
                String[] nestedKeys = key.split("\\.");
                String[] newKeys = Arrays.copyOfRange(nestedKeys, containEdges ? 2 : 1, nestedKeys.length);
                Object value = getValue(account, newKeys, null);
                if (value instanceof JSONArray) {
                    nestedArrSizes.add(((JSONArray) value).length());
                }
            }

            // Maximum size of any nested JSONArray
            int maxSize = nestedArrSizes.stream()
                .mapToInt(Integer::intValue)
                .max()
                .orElse(1);

            for (int j = 0; j < maxSize; j++) {
                int keysIndex = 0;
                // Write data for the account
                List<String> values = new ArrayList<>();

                for (String key : keys) {
                    String[] nestedKeys = key.split("\\.");
                    String[] newKeys = Arrays.copyOfRange(nestedKeys, containEdges ? 2 : 1, nestedKeys.length);
                    Object value = getValue(account, newKeys, j);
                    values.add((value != null && !value.toString().equals("null")) ? "\"" + value.toString() + "\"" : "\"\"");

                    // Check if the current key is accountId
                    if (accountId == null && nestedKeys != null && nestedKeys.length > 0 && nestedKeys[nestedKeys.length - 1].equals(currentFilter)) {
                        accountId = (value != null) ? value.toString() : null;
                        accountsFound.put(accountId, true);
                    }

                    // if last iteration of for loop add "insert" or "delete"
                    if (keysIndex == keys.size() - 1) {
                        String eventType = (accountId != null) ? (String) eventTypeMap.getOrDefault(accountId, "") : "";
                        values.add("\"" + eventType + "\"");
                    }
                    keysIndex++;
                }

                // Print values
                System.out.println(String.join("\t", values));
                stringWriter.write(String.join("\t", values));
                stringWriter.write("\n");

                writer.write(String.join(",", values));
                writer.newLine();
            }
        }

        addHardDelete(stringWriter, keys, eventTypeMap, accountsFound);
        stringWriter.close();
        writer.close();
        String result = stringWriter.toString();
        updateStatus(loadDto, "SUCCESS");
        System.out.println("Data has been written to " + queryName + ".csv");

        // Print the result
        System.out.println("Result:\n" + result);

        containEdges = false;
        eventTypeMap.clear();
        return result;

    } catch (IOException e) {
        updateStatus(loadDto, "FAILED");
        return e.getMessage();
    }
}
```

### Step 3: Modify the `incrementalLoad` Method
Update the `incrementalLoad` method to use the `processInBatches` method.

```java
@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {
    LoadDto loadDto = insertLoad(queryName);

    // Step 1: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }

    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);

    // Step 2: Get Filter from Notification table
    Map<String, Object> filters = getFilters(entityAndFilterColumn);

    // Step 3: Get the eventType map from the notification table
    Map<String, String> eventTypeMap = getEventTypeMap();
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS");
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // Step 4: Process in batches
    String graphQLQuery = radixQueryProperties.get().getRadixQuery();
    String currentFilter = getCurrentFilter(entityAndFilterColumn);
    List<String> headers = extractKeysFromQuery(graphQLQuery, queryName);
    String result = processInBatches(graphQLQuery, filters, eventTypeMap, loadDto, queryName, currentFilter, headers);

    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(result.getBytes()).build();
}
```

### Summary
- **Batch Processing:** The new `processInBatches` method handles the batching of filters, firing the GraphQL queries in batches of 100 IDs.
- **Header Management:** The `storeDataInTXT` method is modified to write headers only for the first batch.
- **Original Logic:** The original business logic is preserved and extended to handle large datasets efficiently.

This approach should help process large datasets while maintaining the integrity of the original implementation.
