Here is the `incrementalLoad` method with the added code for partitioning IDs into sub-lists of 100 and processing them in batches. I've preserved the original code arrangement and added the necessary logic:

```java
@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {

    LoadDto loadDto = null;
    String graphQLResponse = null; //insert record into load table
    loadDto = insertLoad(queryName);
    //STEP-1 : Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }
    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);
    //STEP-2 : Get Filter from Notification table
    Map<String, Object> filters = getFilters(entityAndFilterColumn);
    //Utility method later // this might not be required
    String currentFilter = getCurrentFilter(entityAndFilterColumn);
    //get the eventType map from the notification table
    Map<String, String> eventTypeMap = getEventTypeMap();
    // Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS"); // need to check if this is the right place to update this.
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }
    //STEP-4 : Execute the query
    String graphQLQuery = radixQueryProperties.get().getRadixQuery();

    // Prepare sublists for filters to ensure only 100 IDs are processed at a time
    List<List<String>> subLists = prepareSubLists(new HashSet<>(filters.values()), 100);

    for (List<String> subList : subLists) {
        Map<String, Object> subListFilters = new HashMap<>(filters);
        subListFilters.put(entityAndFilterColumn.values().iterator().next(), subList);

        ResponseEntity<GraphqlResponse> response = baseService.getResponse(graphQLQuery, subListFilters);

        //code to get the operation name from the graphql query //Regular expression pattern to match the operation name
        Pattern pattern = Pattern.compile("^(\\s*(query|mutation|subscription)\\s+(\\w+).*)", Pattern.DOTALL);
        Matcher matcher = pattern.matcher(graphQLQuery);
        String operationName = "";
        if (matcher.find()) {
            String operationType = matcher.group(1);
            operationName = matcher.group(2);
            System.out.println("Operation Type: " + operationType);
            System.out.println("Operation Name: " + operationName);
        } else {
            System.out.println("Operation name not found.");
        }

        List<String> headers = extractKeysFromQuery(graphQLQuery, operationName);
        // System.out.println(headers);
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }
        // StringWriter stringWriter = new StringWriter();
        String result = storeDataInTXT(graphQLResponse, headers, eventTypeMap, loadDto, operationName, currentFilter);
    }

    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(graphQLResponse.getBytes()).build();
}

// Adjusted prepareSubLists method to fit this class
private List<List<String>> prepareSubLists(Set<String> allAccountCodes, int maxSize) throws BusinessException { // New Method
    if (allAccountCodes == null || allAccountCodes.isEmpty()) {
        throw new BusinessException("Account code list must not be empty");
    }
    List<String> accountCodesList = new ArrayList<>(allAccountCodes);
    List<List<String>> subLists = new ArrayList<>();
    for (int i = 0; i < accountCodesList.size(); i += maxSize) {
        subLists.add(accountCodesList.subList(i, Math.min(i + maxSize, accountCodesList.size())));
    }
    return subLists;
}
```

### Changes Made:
1. **Added Partition Logic for Filters:**
   ```java
   // Prepare sublists for filters to ensure only 100 IDs are processed at a time
   List<List<String>> subLists = prepareSubLists(new HashSet<>(filters.values()), 100);

   for (List<String> subList : subLists) {
       Map<String, Object> subListFilters = new HashMap<>(filters);
       subListFilters.put(entityAndFilterColumn.values().iterator().next(), subList);

       ResponseEntity<GraphqlResponse> response = baseService.getResponse(graphQLQuery, subListFilters);
       // ... (rest of the code remains unchanged within this loop)
   }
   ```

2. **New Method `prepareSubLists`:**
   ```java
   private List<List<String>> prepareSubLists(Set<String> allAccountCodes, int maxSize) throws BusinessException {
       if (allAccountCodes == null || allAccountCodes.isEmpty()) {
           throw new BusinessException("Account code list must not be empty");
       }
       List<String> accountCodesList = new ArrayList<>(allAccountCodes);
       List<List<String>> subLists = new ArrayList<>();
       for (int i = 0; i < accountCodesList.size(); i += maxSize) {
           subLists.add(accountCodesList.subList(i, Math.min(i + maxSize, accountCodesList.size())));
       }
       return subLists;
   }
   ```

These additions ensure that only 100 IDs are processed at a time, preventing issues related to exceeding the limit on the number of IDs in a single query. The rest of the original code arrangement remains unchanged.
