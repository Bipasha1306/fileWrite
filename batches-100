public String storeDataInTXT(String jsonResponse, List<String> keys, Map<String, String> eventTypeMap,
                             LoadDto loadDto, String queryName, final String currentFilter, boolean writeHeaders) {
    try (StringWriter stringWriter = new StringWriter()) {
        // Parse JSON data
        JSONObject jsonObject = new JSONObject(jsonResponse);
        JSONArray jsonArray;
        if (containEdges) {
            jsonArray = jsonObject.getJSONObject("data").getJSONObject(queryName).getJSONArray("edges");
        } else {
            jsonArray = jsonObject.getJSONObject("data").getJSONArray(queryName);
        }

        Map<String, Boolean> accountsFound = new HashMap<>();

        // Create a new list for headers
        List<String> headers = new ArrayList<>(keys);
        headers.add("eventType");

        BufferedWriter writer = new BufferedWriter(new FileWriter(queryName + ".csv", true)); // Open in append mode

        // Only write headers if the flag is true
        if (writeHeaders) {
            stringWriter.write(String.join("\t", headers));
            stringWriter.write("\n");
            writer.write(String.join(",", headers));
            writer.newLine();
        }

        // Iterate over the accounts in the JSON response
        for (int i = 0; i < jsonArray.length(); i++) {
            JSONObject account = jsonArray.getJSONObject(i);
            String accountId = null;
            List<Integer> nestedArrSizes = new ArrayList<>();

            // Get the size of all the nested JSONArray present inside this account
            for (String key : keys) {
                String[] nestedKeys = key.split("\\.");
                String[] newKeys = Arrays.copyOfRange(nestedKeys, containEdges ? 2 : 1, nestedKeys.length);
                Object value = getValue(account, newKeys, null);
                if (value instanceof JSONArray) {
                    nestedArrSizes.add(((JSONArray) value).length());
                }
            }

            // Maximum size of any nested JSONArray
            int maxSize = nestedArrSizes.stream()
                .mapToInt(Integer::intValue)
                .max()
                .orElse(1);

            for (int j = 0; j < maxSize; j++) {
                int keysIndex = 0;
                // Write data for the account
                List<String> values = new ArrayList<>();

                for (String key : keys) {
                    String[] nestedKeys = key.split("\\.");
                    String[] newKeys = Arrays.copyOfRange(nestedKeys, containEdges ? 2 : 1, nestedKeys.length);
                    Object value = getValue(account, newKeys, j);
                    values.add((value != null && !value.toString().equals("null")) ? "\"" + value.toString() + "\"" : "\"\"");

                    // Check if the current key is accountId
                    if (accountId == null && nestedKeys != null && nestedKeys.length > 0 && nestedKeys[nestedKeys.length - 1].equals(currentFilter)) {
                        accountId = (value != null) ? value.toString() : null;
                        accountsFound.put(accountId, true);
                    }

                    // if last iteration of for loop add "insert" or "delete"
                    if (keysIndex == keys.size() - 1) {
                        String eventType = (accountId != null) ? (String) eventTypeMap.getOrDefault(accountId, "") : "";
                        values.add("\"" + eventType + "\"");
                    }
                    keysIndex++;
                }

                // Write data to the file and StringWriter
                stringWriter.write(String.join("\t", values));
                stringWriter.write("\n");

                writer.write(String.join(",", values));
                writer.newLine();
            }
        }

        // Call addHardDelete to add any necessary hard delete logic
        addHardDelete(stringWriter, keys, eventTypeMap, accountsFound);

        // Close the writer after processing
        writer.close();

        // Return the result after processing
        String result = stringWriter.toString();
        updateStatus(loadDto, "SUCCESS");
        System.out.println("Data has been written to " + queryName + ".csv");

        // Print the result
        System.out.println("Result:\n" + result);

        // Reset flags and clear the map
        containEdges = false;
        eventTypeMap.clear();
        
        return result;

    } catch (IOException e) {
        // Handle exceptions
        updateStatus(loadDto, "FAILED");
        return e.getMessage();
    }
}

///////////////////
@Service
public class PandaAccountService {

    @Autowired
    private BaseService baseService;
    @Autowired
    private NotificationRepository notificationRepository;

    private final Map<String, String> eventTypemap = new HashMap<>();

    @Transactional
    public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {

        LoadDto loadDto = null;
        String graphQLResponse = null;
        loadDto = insertLoad(queryName);

        // STEP-1: Get query from spring cloud config
        Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
        if(radixQueryProperties.isEmpty()){
            throw new BusinessException("Query name is incorrect");
        }

        Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);
        
        // STEP-2: Get Filter from Notification table
        Map<String, Object> filters = getFilters(entityAndFilterColumn);

        // Get the eventType map from the notification table
        Map<String, String> eventTypeMap = getEventTypeMap();
        
        if (eventTypeMap.isEmpty()) {
            updateStatus(loadDto, "SUCCESS");
            return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
        }

        // STEP-4: Execute the query in batches
        String graphQLQuery = radixQueryProperties.get().getRadixQuery();
        
        // Create a flag to control header writing
        boolean writeHeaders = true;
        StringBuilder resultBuilder = new StringBuilder();
        
        List<List<String>> subListFilters = createSublists((List<String>) filters.get(entityAndFilterColumn.get("filterKey")), 100);
        for (List<String> sublist : subListFilters) {
            ResponseEntity<GraphqlResponse> response = baseService.getResponse(graphQLQuery, sublist);
            String graphQLResponseBatch;
            
            try {
                graphQLResponseBatch = new ObjectMapper().writeValueAsString(response.getBody());
            } catch (JsonProcessingException e) {
                throw new RuntimeException(e);
            }
            
            // Call the storeDataInTXT method for each batch, passing the flag to control headers
            String batchResult = storeDataInTXT(graphQLResponseBatch, extractKeysFromQuery(graphQLQuery, ""), eventTypeMap, loadDto, queryName, writeHeaders);
            resultBuilder.append(batchResult);
            
            // After the first batch, headers should no longer be written
            writeHeaders = false;
        }

        // Return final result after all batches are processed
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(resultBuilder.toString().getBytes()).build();
    }

    // Your existing methods here...

    // Batch creation method
    private List<List<String>> createSublists(List<String> list, int batchSize) {
        List<List<String>> sublists = new ArrayList<>();
        for (int i = 0; i < list.size(); i += batchSize) {
            int end = Math.min(i + batchSize, list.size());
            sublists.add(new ArrayList<>(list.subList(i, end)));
        }
        return sublists;
    }
//////////////////////
@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {
    LoadDto loadDto = null;
    String graphQLResponse = null;
    loadDto = insertLoad(queryName); // Insert record into the load table

    // STEP-1: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }

    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);

    // STEP-2: Get filters from Notification table
    Map<String, Object> filters = getFilters(entityAndFilterColumn);
    String currentFilter = getCurrentFilter(entityAndFilterColumn); // Utility method to retrieve current filter

    // Get eventTypeMap from the Notification table
    Map<String, String> eventTypeMap = getEventTypeMap();

    // Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS"); // Confirm if this is the right place to update
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // STEP-3: Create sublists from filters and eventTypeMap and process them in batches
    Map<String, List<String>> filtersByEntity = prepareFiltersByEntity(filters); // Assuming filters are entity-specific
    List<List<String>> sublists = createSublistsFromFiltersAndEventTypeMap(eventTypeMap, filtersByEntity);

    StringBuilder finalResult = new StringBuilder();
    boolean writeHeaders = true; // Ensure headers are written only once

    for (List<String> sublistFilters : sublists) {
        // Modify filters map to use sublistFilters
        Map<String, Object> modifiedFilters = new HashMap<>(filters);
        modifiedFilters.put(currentFilter, sublistFilters);

        // STEP-4: Execute the query for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(radixQueryProperties.get().getRadixQuery(), modifiedFilters);

        // Serialize the GraphQL response to a string
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // STEP-5: Call storeDataInTXT for each batch and append the results
        String result = storeDataInTXT(graphQLResponse, extractKeysFromQuery(radixQueryProperties.get().getRadixQuery(), queryName), 
                                       eventTypeMap, loadDto, queryName, currentFilter, writeHeaders);
        finalResult.append(result);

        // Set writeHeaders to false after the first batch
        writeHeaders = false;
    }

    // STEP-6: Return the final accumulated result
    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(finalResult.toString().getBytes()).build();
}

public List<List<String>> createSublistsFromFiltersAndEventTypeMap(Map<String, String> eventTypeMap, Map<String, List<String>> filters) {
    // Initialize a list to store the sublists
    List<String> matchedValues = new ArrayList<>();

    // Loop through the eventTypeMap keys and match them with the filters
    for (String eventKey : eventTypeMap.keySet()) {
        // Assuming the filters map contains a list of values for each event key
        List<String> filterValues = filters.get(eventKey);

        if (filterValues != null) {
            // Add only unique values to the matchedValues list
            matchedValues.addAll(filterValues.stream().distinct().collect(Collectors.toList()));
        }
    }

    // Now that we have the matched and unique values, let's break them into sublists
    return createSublists(matchedValues, 100);
}

// Helper function to create sublists
private List<List<String>> createSublists(List<String> list, int batchSize) {
    List<List<String>> sublists = new ArrayList<>();

    // Split the list into sublists of the given batch size
    for (int i = 0; i < list.size(); i += batchSize) {
        int end = Math.min(i + batchSize, list.size());
        sublists.add(new ArrayList<>(list.subList(i, end)));
    }

    return sublists;
}
//////////////////////---
@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {
    // STEP-1: Insert record into the load table
    LoadDto loadDto = insertLoad(queryName); 

    // STEP-2: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }

    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);

    // STEP-3: Get filters from Notification table
    Map<String, Object> filters = getFilters(entityAndFilterColumn);
    String currentFilter = getCurrentFilter(entityAndFilterColumn); // Utility method to retrieve current filter

    // Get eventTypeMap from the Notification table
    Map<String, String> eventTypeMap = getEventTypeMap();

    // STEP-4: Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS"); // Confirm if this is the right place to update
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // Extract operation name from the GraphQL query
    String graphQLQuery = radixQueryProperties.get().getRadixQuery();
    Pattern pattern = Pattern.compile("^(\\s*(query|mutation|subscription)\\s+(\\w+).*)", Pattern.DOTALL);
    Matcher matcher = pattern.matcher(graphQLQuery);
    String operationName = "";
    if (matcher.find()) {
        operationName = matcher.group(3); // Correctly extract the operation name
        System.out.println("Operation Type: " + matcher.group(2));
        System.out.println("Operation Name: " + operationName);
    } else {
        System.out.println("Operation name not found.");
    }

    // STEP-5: Process the filters and eventTypeMap in batches
    Map<String, List<String>> filtersByEntity = prepareFiltersByEntity(filters); // Assuming filters are entity-specific
    List<List<String>> sublists = createSublistsFromFiltersAndEventTypeMap(eventTypeMap, filtersByEntity);

    StringBuilder finalResult = new StringBuilder();
    boolean writeHeaders = true; // Ensure headers are written only once

    for (List<String> sublistFilters : sublists) {
        // Modify filters map to use sublistFilters
        Map<String, Object> modifiedFilters = new HashMap<>(filters);
        modifiedFilters.put(currentFilter, sublistFilters);

        // STEP-6: Execute the query for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(graphQLQuery, modifiedFilters);

        // Serialize the GraphQL response to a string
        ObjectMapper objectMapper = new ObjectMapper();
        String graphQLResponse;
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // STEP-7: Call storeDataInTXT for each batch and append the results
        List<String> headers = extractKeysFromQuery(graphQLQuery, operationName); // Include operation name in headers extraction
        String result = storeDataInTXT(graphQLResponse, headers, eventTypeMap, loadDto, queryName, currentFilter, writeHeaders);
        finalResult.append(result);

        // Set writeHeaders to false after the first batch
        writeHeaders = false;
    }

    // STEP-8: Return the final accumulated result
    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(finalResult.toString().getBytes()).build();
}

public List<List<String>> createSublistsFromFiltersAndEventTypeMap(Map<String, String> eventTypeMap, Map<String, List<String>> filters) {
    // Initialize a list to store the sublists
    List<String> matchedValues = new ArrayList<>();

    // Loop through the eventTypeMap keys and match them with the filters
    for (String eventKey : eventTypeMap.keySet()) {
        // Assuming the filters map contains a list of values for each event key
        List<String> filterValues = filters.get(eventKey);

        if (filterValues != null) {
            // Add only unique values to the matchedValues list
            matchedValues.addAll(filterValues.stream().distinct().collect(Collectors.toList()));
        }
    }

    // Now that we have the matched and unique values, let's break them into sublists
    return createSublists(matchedValues, 100);
}

// Helper function to create sublists
private List<List<String>> createSublists(List<String> list, int batchSize) {
    List<List<String>> sublists = new ArrayList<>();

    // Split the list into sublists of the given batch size
    for (int i = 0; i < list.size(); i += batchSize) {
        int end = Math.min(i + batchSize, list.size());
        sublists.add(new ArrayList<>(list.subList(i, end)));
    }

    return sublists;
}


/////////////////////////
Apologies for the confusion! I'll keep the existing logic intact and just add the extra logic for processing in batches and handling headers.

Here’s the complete approach:

### Step-by-Step Plan

1. **Batching Logic**: We’ll split the filters into batches of 100 and call the existing `storeDataInTXT` method for each batch.
2. **Header Control**: Ensure headers are written only once, during the first batch.
3. **Accumulate Results**: Collect the results from each batch and return the complete result in the `PandaRunStatusDto`.

Here’s the full integration with the original `PandaRunStatusDto` logic and `storeDataInTXT` method:

### Code Implementation

```java
@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {

    LoadDto loadDto = null;
    String graphQLResponse = null; // Insert record into load table
    loadDto = insertLoad(queryName);

    // STEP-1: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }
    
    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);

    // STEP-2: Get filters from the Notification table
    Map<String, Object> filters = getFilters(entityAndFilterColumn);
    String currentFilter = getCurrentFilter(entityAndFilterColumn);

    // Get the eventType map from the notification table
    Map<String, String> eventTypeMap = getEventTypeMap();

    // Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS");
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // STEP-4: Execute the query
    String graphQLQuery = radixQueryProperties.get().getRadixQuery();
    Pattern pattern = Pattern.compile("^(\\s*(query|mutation|subscription)\\s+(\\w+).*)", Pattern.DOTALL);
    Matcher matcher = pattern.matcher(graphQLQuery);
    String operationName = "";

    if (matcher.find()) {
        operationName = matcher.group(2);
    }

    List<String> headers = extractKeysFromQuery(graphQLQuery, operationName);

    // Splitting filters into sublists of 100
    List<List<String>> filterSublists = createSublists(filters.get(currentFilter), 100);

    // StringBuilder to accumulate the result of all batches
    StringBuilder finalResult = new StringBuilder();

    boolean writeHeaders = true;  // Ensure headers are written only for the first batch

    for (List<String> subList : filterSublists) {
        // Prepare filters for the current batch
        Map<String, Object> subListFilters = new HashMap<>(filters);
        subListFilters.put(currentFilter, subList);

        // Fetch data for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(graphQLQuery, subListFilters);

        // Convert response to JSON string
        try {
            graphQLResponse = new ObjectMapper().writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // Store data for the current batch
        String result = storeDataInTXT(graphQLResponse, headers, eventTypeMap, loadDto, operationName, currentFilter, writeHeaders);

        // Append the result to finalResult
        finalResult.append(result);

        // Set writeHeaders to false after the first batch
        writeHeaders = false;
    }

    // Return the complete result in PandaRunStatusDto
    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(finalResult.toString().getBytes()).build();
}
```

### Modified `storeDataInTXT` Method (Adding `writeHeaders` Flag)

```java
public String storeDataInTXT(String jsonResponse, List<String> keys, Map<String, String> eventTypeMap,
                             LoadDto loadDto, String queryName, final String currentFilter, boolean writeHeaders) {
    try (StringWriter stringWriter = new StringWriter()) {
        // Parse JSON data
        JSONObject jsonObject = new JSONObject(jsonResponse);
        JSONArray jsonArray;
        if (containEdges) {
            jsonArray = jsonObject.getJSONObject("data").getJSONObject(queryName).getJSONArray("edges");
        } else {
            jsonArray = jsonObject.getJSONObject("data").getJSONArray(queryName);
        }

        Map<String, Boolean> accountsFound = new HashMap<>();

        // Create a new list for headers
        List<String> headers = new ArrayList<>(keys);
        headers.add("eventType");

        BufferedWriter writer = new BufferedWriter(new FileWriter(queryName + ".csv", true)); // Open in append mode

        // Only write headers if the flag is true
        if (writeHeaders) {
            stringWriter.write(String.join("\t", headers));
            stringWriter.write("\n");
            writer.write(String.join(",", headers));
            writer.newLine();
        }

        // Iterate over the accounts in the JSON response
        for (int i = 0; i < jsonArray.length(); i++) {
            JSONObject account = jsonArray.getJSONObject(i);
            String accountId = null;
            List<Integer> nestedArrSizes = new ArrayList<>();

            // Get the size of all the nested JSONArray present inside this account
            for (String key : keys) {
                String[] nestedKeys = key.split("\\.");
                String[] newKeys = Arrays.copyOfRange(nestedKeys, containEdges ? 2 : 1, nestedKeys.length);
                Object value = getValue(account, newKeys, null);
                if (value instanceof JSONArray) {
                    nestedArrSizes.add(((JSONArray) value).length());
                }
            }

            // Maximum size of any nested JSONArray
            int maxSize = nestedArrSizes.stream()
                .mapToInt(Integer::intValue)
                .max()
                .orElse(1);

            for (int j = 0; j < maxSize; j++) {
                int keysIndex = 0;
                // Write data for the account
                List<String> values = new ArrayList<>();

                for (String key : keys) {
                    String[] nestedKeys = key.split("\\.");
                    String[] newKeys = Arrays.copyOfRange(nestedKeys, containEdges ? 2 : 1, nestedKeys.length);
                    Object value = getValue(account, newKeys, j);
                    values.add((value != null && !value.toString().equals("null")) ? "\"" + value.toString() + "\"" : "\"\"");

                    // Check if the current key is accountId
                    if (accountId == null && nestedKeys != null && nestedKeys.length > 0 && nestedKeys[nestedKeys.length - 1].equals(currentFilter)) {
                        accountId = (value != null) ? value.toString() : null;
                        accountsFound.put(accountId, true);
                    }

                    // if last iteration of for loop add "insert" or "delete"
                    if (keysIndex == keys.size() - 1) {
                        String eventType = (accountId != null) ? (String) eventTypeMap.getOrDefault(accountId, "") : "";
                        values.add("\"" + eventType + "\"");
                    }
                    keysIndex++;
                }

                // Write data to the file and StringWriter
                stringWriter.write(String.join("\t", values));
                stringWriter.write("\n");

                writer.write(String.join(",", values));
                writer.newLine();
            }
        }

        // Call addHardDelete to add any necessary hard delete logic
        addHardDelete(stringWriter, keys, eventTypeMap, accountsFound);

        // Close the writer after processing
        writer.close();

        // Return the result after processing
        String result = stringWriter.toString();
        updateStatus(loadDto, "SUCCESS");
        System.out.println("Data has been written to " + queryName + ".csv");

        // Reset flags and clear the map
        containEdges = false;
        eventTypeMap.clear();
        
        return result;

    } catch (IOException e) {
        // Handle exceptions
        updateStatus(loadDto, "FAILED");
        return e.getMessage();
    }
}
```

### Explanation:

1. **Batch Processing in `incrementalLoad`**: 
   - We split the filters into sublists of 100 elements each using the `createSublists` method. 
   - For each sublist, we call the existing `storeDataInTXT` method to process and store the results.
   - The headers are written only once, in the first call of `storeDataInTXT`.

2. **Final Result Accumulation**: 
   - We accumulate all results in a `StringBuilder`, which is then returned in the final `PandaRunStatusDto` object.

3. **Header Control in `storeDataInTXT`**:
   - We introduced a `writeHeaders` flag that ensures headers are written only once, during the first batch. For subsequent batches, the flag is set to `false`, so headers are not duplicated.

This way, the original logic stays intact, and the additional logic for batching and header control is seamlessly integrated.

Let me know if this works for you!
////////////////////////////////////
import org.springframework.http.ResponseEntity;
import org.springframework.transaction.annotation.Transactional;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.core.JsonProcessingException;

import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {
    LoadDto loadDto = null;
    String graphQLResponse = null;
    
    // Insert record into the load table
    loadDto = insertLoad(queryName);

    // STEP-1: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }

    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);

    // STEP-2: Get filters from Notification table
    Map<String, Object> filters = getFilters(entityAndFilterColumn);
    String currentFilter = getCurrentFilter(entityAndFilterColumn); // Utility method

    // Get the eventTypeMap from the Notification table
    Map<String, String> eventTypeMap = getEventTypeMap();

    // Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS"); // Confirm if this is the right place to update
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // STEP-3: Create sublists from eventTypeMap keys
    List<String> eventTypeKeys = new ArrayList<>(eventTypeMap.keySet());
    List<List<String>> sublists = createSublists(eventTypeKeys, 100);

    // Process each sublist
    StringBuilder finalResult = new StringBuilder();
    boolean writeHeaders = true; // Ensure headers are written only once

    for (List<String> sublist : sublists) {
        // Modify filters map to use current sublist
        Map<String, Object> modifiedFilters = new HashMap<>(filters);
        modifiedFilters.put(currentFilter, sublist);

        // STEP-4: Execute the query for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(radixQueryProperties.get().getRadixQuery(), modifiedFilters);
        String graphQLQuery = radixQueryProperties.get().getRadixQuery();
        
        // Code to get the operation name from the GraphQL query
        Pattern pattern = Pattern.compile("^(\\s*(query|mutation|subscription)\\s+(\\w+).*)", Pattern.DOTALL);
        Matcher matcher = pattern.matcher(graphQLQuery);
        String operationName = "";
        if (matcher.find()) {
            String operationType = matcher.group(1);
            operationName = matcher.group(2);
            System.out.println("Operation Type: " + operationType);
            System.out.println("Operation Name: " + operationName);
        } else {
            System.out.println("Operation name not found.");
        }

        List<String> headers = extractKeysFromQuery(graphQLQuery, operationName);

        // Serialize the GraphQL response to a string
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // STEP-5: Call storeDataInTXT for each batch and append the results
        String result = storeDataInTXT(graphQLResponse, headers, eventTypeMap, loadDto, operationName, currentFilter);
        finalResult.append(result);

        // Set writeHeaders to false after the first batch
        writeHeaders = false;
    }

    // STEP-6: Return the final accumulated result
    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(finalResult.toString().getBytes()).build();
}

// Helper function to create sublists
private List<List<String>> createSublists(List<String> list, int batchSize) {
    List<List<String>> sublists = new ArrayList<>();
    for (int i = 0; i < list.size(); i += batchSize) {
        int end = Math.min(i + batchSize, list.size());
        sublists.add(new ArrayList<>(list.subList(i, end)));
    }
    return sublists;
}
/////////////////////  -11:34 pm
@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {
    LoadDto loadDto = null;
    String graphQLResponse = null;

    // Insert record into the load table
    loadDto = insertLoad(queryName);

    // STEP-1: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }

    // Get the eventTypeMap from the Notification table
    Map<String, String> eventTypeMap = getEventTypeMap();

    // Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS"); // Confirm if this is the right place to update
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // STEP-3: Create sublists from eventTypeMap keys
    List<String> eventTypeKeys = new ArrayList<>(eventTypeMap.keySet());
    List<List<String>> sublists = createSublists(eventTypeKeys, 100);

    // Process each sublist
    StringBuilder finalResult = new StringBuilder();
    boolean writeHeaders = true; // Ensure headers are written only once

    for (List<String> sublist : sublists) {
        // STEP-4: Execute the query for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(radixQueryProperties.get().getRadixQuery(), sublist);

        String graphQLQuery = radixQueryProperties.get().getRadixQuery();

        // Code to get the operation name from the GraphQL query
        Pattern pattern = Pattern.compile("^(\\s*(query|mutation|subscription)\\s+(\\w+).*)", Pattern.DOTALL);
        Matcher matcher = pattern.matcher(graphQLQuery);
        String operationName = "";
        if (matcher.find()) {
            operationName = matcher.group(2);
        }

        List<String> headers = extractKeysFromQuery(graphQLQuery, operationName);

        // Serialize the GraphQL response to a string
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // STEP-5: Call storeDataInTXT for each batch and append the results
        String result = storeDataInTXT(graphQLResponse, headers, eventTypeMap, loadDto, operationName, null);
        finalResult.append(result);

        // Set writeHeaders to false after the first batch
        writeHeaders = false;
    }

    // STEP-6: Return the final accumulated result
    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(finalResult.toString().getBytes()).build();
}

// Helper function to create sublists
private List<List<String>> createSublists(List<String> list, int batchSize) {
    List<List<String>> sublists = new ArrayList<>();
    for (int i = 0; i < list.size(); i += batchSize) {
        int end = Math.min(i + batchSize, list.size());
        sublists.add(new ArrayList<>(list.subList(i, end)));
    }
    return sublists;
}
//////////11:58

@Transactional
public PandaRunStatusDto incrementalLoad(String queryName) throws BusinessException {
    LoadDto loadDto = null;
    String graphQLResponse = null; 
    
    // Insert record into the load table
    loadDto = insertLoad(queryName);

    // STEP-1: Get query from spring cloud config
    Optional<PandaDataRadixConfigProperties.RadixQueryProperties> radixQueryProperties = baseService.getRadixQueryProperties(queryName);
    if (radixQueryProperties.isEmpty()) {
        throw new BusinessException("Query name is incorrect");
    }

    Map<String, String> entityAndFilterColumn = baseService.getEntitiesAndFilterColumnsByQuery(radixQueryProperties);

    // STEP-2: Get the eventTypeMap from the Notification table
    Map<String, String> eventTypeMap = getEventTypeMap();

    // Return empty values if eventTypeMap is empty
    if (eventTypeMap.isEmpty()) {
        updateStatus(loadDto, "SUCCESS");
        return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(null).build();
    }

    // STEP-3: Create sublists from eventTypeMap keys
    List<String> eventTypeKeys = new ArrayList<>(eventTypeMap.keySet());
    List<List<String>> sublists = createSublists(eventTypeKeys, 100);

    // Process each sublist
    StringBuilder finalResult = new StringBuilder();
    boolean writeHeaders = true;

    for (List<String> sublist : sublists) {
        // Create modified filters using eventTypeMap for the current sublist
        Map<String, Object> modifiedFilters = new HashMap<>();
        modifiedFilters.put(currentFilter, sublist);

        // STEP-4: Execute the query for the current batch
        ResponseEntity<GraphqlResponse> response = baseService.getResponse(radixQueryProperties.get().getRadixQuery(), modifiedFilters);
        String graphQLQuery = radixQueryProperties.get().getRadixQuery();
        
        // Extract operation name from the GraphQL query
        Pattern pattern = Pattern.compile("^(\\s*(query|mutation|subscription)\\s+(\\w+).*)", Pattern.DOTALL);
        Matcher matcher = pattern.matcher(graphQLQuery);
        String operationName = "";
        if (matcher.find()) {
            String operationType = matcher.group(1);
            operationName = matcher.group(2);
            System.out.println("Operation Type: " + operationType);
            System.out.println("Operation Name: " + operationName);
        } else {
            System.out.println("Operation name not found.");
        }

        List<String> headers = extractKeysFromQuery(graphQLQuery, operationName);

        // Serialize the GraphQL response to a string
        ObjectMapper objectMapper = new ObjectMapper();
        try {
            graphQLResponse = objectMapper.writeValueAsString(response.getBody());
        } catch (JsonProcessingException e) {
            throw new RuntimeException(e);
        }

        // STEP-5: Call storeDataInTXT for each batch and append the results
        String result = storeDataInTXT(graphQLResponse, headers, eventTypeMap, loadDto, operationName, currentFilter);
        finalResult.append(result);

        // Set writeHeaders to false after the first batch
        writeHeaders = false;
    }

    // STEP-6: Return the final accumulated result
    return PandaRunStatusDto.builder().runId(loadDto.getRunId()).fileContent(finalResult.toString().getBytes()).build();
}

// Helper function to create sublists
private List<List<String>> createSublists(List<String> list, int batchSize) {
    List<List<String>> sublists = new ArrayList<>();
    for (int i = 0; i < list.size(); i += batchSize) {
        int end = Math.min(i + batchSize, list.size());
        sublists.add(new ArrayList<>(list.subList(i, end)));
    }
    return sublists;
}
